{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d314bbdf6b76483984fdcf450b8de222",
            "a74d9e5e6d87408c85f09eecac379f08",
            "740e35c7cf814a1587a91c49a840b8c8",
            "7423c0986ab64d608d46e6f89bbfccbd",
            "5e01fd437b2541899bc494962ac8d5a5",
            "c4376bac09d645ee86398874967a7228",
            "6d752b2e912340648faac4320a1fad2a",
            "361d74fac40741968ff1c2f9bb47b151"
          ]
        },
        "id": "GOXGFJwlnvok",
        "outputId": "433336c0-4f56-4055-83b8-51283f147403"
      },
      "outputs": [],
      "source": [
        "# Part 1: Imports and Model Architecture\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import StratifiedGroupKFold, StratifiedShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple, List\n",
        "from gemelli.preprocessing import matrix_rclr\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "class NormalizedTransformerBlock(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super(NormalizedTransformerBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=4, dropout=0, batch_first=True)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GeLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "        )\n",
        "        self.alphaA = nn.Parameter(torch.tensor(1.0))\n",
        "        self.alphaM = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Normalize input\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "        # Attention block\n",
        "        hA, _ = self.attention(x, x, x)\n",
        "        hA = F.normalize(hA, p=2, dim=-1)\n",
        "        x = F.normalize(x + self.alphaA * (hA - x), p=2, dim=-1)\n",
        "\n",
        "        # MLP block\n",
        "        hM = self.mlp(x)\n",
        "        hM = F.normalize(hM, p=2, dim=-1)\n",
        "        x = F.normalize(x + self.alphaM * (hM - x), p=2, dim=-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MTLNormalizedTransformer(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int, num_countries: int, projection_dim: int = 1):\n",
        "        super(MTLNormalizedTransformer, self).__init__()\n",
        "        self.projection_dim = projection_dim\n",
        "\n",
        "        # Shared backbone\n",
        "        self.pca_projection = nn.Linear(input_dim, hidden_dim)\n",
        "        self.view_generator = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, projection_dim * hidden_dim),\n",
        "            nn.LayerNorm(projection_dim * hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            NormalizedTransformerBlock(hidden_dim, hidden_dim * 2)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Task-specific heads\n",
        "        self.regression_head = nn.Linear(hidden_dim, 1)\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_countries)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Shared processing\n",
        "        x = self.pca_projection(x)\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "        x = self.view_generator(x)\n",
        "        x = x.view(batch_size, self.projection_dim, -1)\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "        # Transformer processing\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Global pooling\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        # Task-specific outputs\n",
        "        regression_output = self.regression_head(x)\n",
        "        classification_logits = self.classification_head(x)\n",
        "\n",
        "        return {\n",
        "            'regression_output': regression_output,\n",
        "            'classification_output': classification_logits\n",
        "        }\n",
        "\n",
        "def calculate_sparsity(model: nn.Module, threshold: float = 1e-5) -> float:\n",
        "    \"\"\"Calculate model sparsity.\"\"\"\n",
        "    def count_sparse_elements(tensor: torch.Tensor, threshold: float) -> float:\n",
        "        return (torch.abs(tensor) < threshold).float().mean().item()\n",
        "\n",
        "    params = [p for p in model.parameters() if p.dim() > 0]\n",
        "    if not params:\n",
        "        return 0.0\n",
        "\n",
        "    sparsities = [count_sparse_elements(p, threshold) for p in params]\n",
        "    return sum(sparsities) / len(sparsities)\n",
        "\n",
        "def calculate_weight_entropy(model: nn.Module, epsilon: float = 1e-10) -> Tuple[float, Dict[str, float]]:\n",
        "    \"\"\"Calculate the absolute weight entropy.\"\"\"\n",
        "    def compute_entropy(tensor: torch.Tensor) -> float:\n",
        "        abs_weights = torch.abs(tensor.flatten())\n",
        "        normalized_weights = abs_weights / (torch.sum(abs_weights) + epsilon)\n",
        "        entropy = -torch.sum(normalized_weights * torch.log(normalized_weights + epsilon)).item()\n",
        "        return entropy\n",
        "\n",
        "    total_entropy = 0.0\n",
        "    layer_entropies = {}\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 0:\n",
        "            layer_entropy = compute_entropy(param)\n",
        "            layer_entropies[name] = layer_entropy\n",
        "            total_entropy += layer_entropy\n",
        "\n",
        "    return total_entropy, layer_entropies\n",
        "\n",
        "class UncertaintyLoss(nn.Module):\n",
        "    def __init__(self, num_tasks=2):\n",
        "        super().__init__()\n",
        "        self.log_vars = nn.Parameter(torch.zeros(num_tasks))\n",
        "\n",
        "    def forward(self, losses):\n",
        "        precision1 = torch.exp(-self.log_vars[0])\n",
        "        precision2 = torch.exp(-self.log_vars[1])\n",
        "        return precision1 * losses[0] + precision2 * losses[1] + self.log_vars.sum()\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(params: Dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Load and preprocess the data files with proper error handling and type conversion.\n",
        "    \"\"\"\n",
        "    # Load the control table\n",
        "    table = pd.read_csv('data/control.csv', index_col=0, low_memory=False)\n",
        "\n",
        "    # Load metadata with proper type handling\n",
        "    age_metadata = pd.read_csv('data/sampleMetadata.csv', index_col='sample_id', low_memory=False)\n",
        "\n",
        "    # Convert age to numeric, setting errors='coerce' will convert non-numeric values to NaN\n",
        "    age_metadata['age'] = pd.to_numeric(age_metadata['age'], errors='coerce')\n",
        "\n",
        "    # Filter out rows where age is NaN\n",
        "    age_metadata = age_metadata[age_metadata['age'].notna()]\n",
        "\n",
        "    # Filter by body site\n",
        "    age_metadata = age_metadata[age_metadata['body_site'] == params['body_site']]\n",
        "\n",
        "    # Ensure we have enough data\n",
        "    if len(age_metadata) < 10:\n",
        "        raise ValueError(f\"Not enough samples for body site {params['body_site']}\")\n",
        "\n",
        "    print(f\"Number of samples after filtering: {len(age_metadata)}\")\n",
        "    print(f\"Age range: {age_metadata['age'].min():.1f} - {age_metadata['age'].max():.1f}\")\n",
        "    print(f\"Number of countries: {age_metadata['country'].nunique()}\")\n",
        "\n",
        "    return table, age_metadata\n",
        "\n",
        "def prepare_mtl_datasets(table: pd.DataFrame, age_metadata: pd.DataFrame,\n",
        "                        params: Dict) -> Tuple[np.ndarray, np.ndarray, np.ndarray, LabelEncoder, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Prepare datasets for multi-task learning with stratification by age category and study.\"\"\"\n",
        "    try:\n",
        "        print(\"\\nInitial data shapes:\")\n",
        "        print(f\"Table shape: {table.shape}\")\n",
        "        print(f\"Metadata shape: {age_metadata.shape}\")\n",
        "\n",
        "        # Handle age processing\n",
        "        age_metadata = age_metadata[age_metadata['age'].notna() & (age_metadata.age > 18)].copy()\n",
        "        print(f\"\\nSamples after removing NaN ages: {len(age_metadata)}\")\n",
        "        print(f\"Age range: {age_metadata['age'].min():.1f} - {age_metadata['age'].max():.1f}\")\n",
        "\n",
        "        # Create age categories\n",
        "        age_metadata['age_category'] = pd.cut(age_metadata['age'],\n",
        "                                             bins=[18, 30, 45, 60, 75, 100],\n",
        "                                             labels=['18-30', '31-45', '46-60', '61-75', '76+'],\n",
        "                                             right=False)\n",
        "\n",
        "        # Handle country data\n",
        "        if 'country' not in age_metadata.columns:\n",
        "            raise ValueError(\"'country' column not found in metadata\")\n",
        "\n",
        "        # Remove rows with missing country data\n",
        "        age_metadata = age_metadata[age_metadata['country'].notna()]\n",
        "\n",
        "        # Get country distribution and select top 5\n",
        "        country_counts = Counter(age_metadata['country'])\n",
        "        top_5_countries = [country for country, _ in country_counts.most_common(5)]\n",
        "        print(f\"\\nSelected top 5 countries: {', '.join(top_5_countries)}\")\n",
        "\n",
        "        # Filter for top 5 countries\n",
        "        mask = age_metadata['country'].isin(top_5_countries)\n",
        "        age_metadata = age_metadata[mask]\n",
        "\n",
        "        # Get abundance data\n",
        "        metadata_cols = ['study_name', 'study_condition', 'subject_id', 'body_site']\n",
        "        abundance_cols = [col for col in table.columns if col not in metadata_cols]\n",
        "        abundance_data = table[abundance_cols].copy()\n",
        "\n",
        "        # Find shared indices\n",
        "        shared_indices = abundance_data.index.intersection(age_metadata.index)\n",
        "        print(f\"\\nShared samples before filtering: {len(shared_indices)}\")\n",
        "\n",
        "        if len(shared_indices) == 0:\n",
        "            raise ValueError(\"No shared samples between abundance data and metadata\")\n",
        "\n",
        "        # Filter data using shared indices\n",
        "        abundance_data = abundance_data.loc[shared_indices]\n",
        "        age_metadata = age_metadata.loc[shared_indices]\n",
        "\n",
        "        # Handle abundance data\n",
        "        abundance_data = abundance_data.apply(pd.to_numeric, errors='coerce')\n",
        "        valid_abundance = ~abundance_data.isna().any(axis=1)\n",
        "        abundance_data = abundance_data[valid_abundance]\n",
        "        age_metadata = age_metadata.loc[abundance_data.index]\n",
        "\n",
        "        print(f\"Samples after removing NaN abundances: {len(abundance_data)}\")\n",
        "\n",
        "        # Get balanced sample sizes per country\n",
        "        country_counts = Counter(age_metadata['country'])\n",
        "        min_samples = min(country_counts.values())\n",
        "        print(f\"\\nBalancing to {min_samples} samples per country\")\n",
        "\n",
        "        # Perform balanced sampling\n",
        "        balanced_indices = []\n",
        "        np.random.seed(42)\n",
        "\n",
        "        for country in top_5_countries:\n",
        "            country_indices = age_metadata[age_metadata['country'] == country].index\n",
        "            if len(country_indices) >= min_samples:\n",
        "                sampled_indices = np.random.choice(country_indices, min_samples, replace=False)\n",
        "                balanced_indices.extend(sampled_indices)\n",
        "\n",
        "        # Apply balanced sampling\n",
        "        age_metadata = age_metadata.loc[balanced_indices]\n",
        "        abundance_data = abundance_data.loc[balanced_indices]\n",
        "\n",
        "        # Create stratification labels\n",
        "        age_metadata['stratify_label'] = (age_metadata['study_name'].astype(str) +\n",
        "                                        \"_\" +\n",
        "                                        age_metadata['age_category'].astype(str) +\n",
        "                                        \"_\" +\n",
        "                                        age_metadata['country'].astype(str))\n",
        "\n",
        "        # Prepare final datasets\n",
        "        X = abundance_data.values\n",
        "        y_age = age_metadata['age'].values.reshape(-1, 1)\n",
        "        subject_ids = age_metadata['subject_id'].values\n",
        "\n",
        "        # Create label encoder for countries\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_country = label_encoder.fit_transform(age_metadata['country'])\n",
        "\n",
        "        # Print final dataset statistics\n",
        "        print(\"\\nFinal balanced dataset statistics:\")\n",
        "        print(f\"Number of features: {X.shape[1]}\")\n",
        "        print(f\"Number of samples: {X.shape[0]}\")\n",
        "        print(f\"Number of countries: {len(label_encoder.classes_)}\")\n",
        "        print(\"Countries and sample counts:\")\n",
        "        for country in label_encoder.classes_:\n",
        "            count = (age_metadata['country'] == country).sum()\n",
        "            print(f\"{country}: {count} samples\")\n",
        "        print(f\"Age range: {y_age.min():.1f} - {y_age.max():.1f}\")\n",
        "        print(f\"Number of stratification classes: {len(np.unique(age_metadata['stratify_label']))}\")\n",
        "\n",
        "        # Verify balance\n",
        "        unique_counts = np.unique(y_country, return_counts=True)[1]\n",
        "        if not np.all(unique_counts == unique_counts[0]):\n",
        "            raise ValueError(\"Dataset is not properly balanced\")\n",
        "\n",
        "        return (X.astype(np.float32),\n",
        "                y_age.astype(np.float32),\n",
        "                y_country,\n",
        "                label_encoder,\n",
        "                age_metadata['stratify_label'].values,\n",
        "                subject_ids)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in prepare_mtl_datasets: {str(e)}\")\n",
        "        print(f\"Table shape: {table.shape}\")\n",
        "        print(f\"Age metadata shape: {age_metadata.shape}\")\n",
        "        raise\n",
        "\n",
        "def prepare_country_data(age_metadata: pd.DataFrame, min_samples: int = 100) -> Tuple[pd.DataFrame, LabelEncoder, np.ndarray]:\n",
        "    \"\"\"Filter and prepare balanced country data with proper index handling.\"\"\"\n",
        "    # Get top 5 countries\n",
        "    country_counts = Counter(age_metadata['country'])\n",
        "\n",
        "    top_countries = [country for country, count in country_counts.most_common()\n",
        "                    if count >= min_samples][:5]\n",
        "\n",
        "\n",
        "    # Filter metadata\n",
        "    filtered_metadata = age_metadata[age_metadata['country'].isin(top_countries)].copy()\n",
        "\n",
        "    # Balance dataset by undersampling with index preservation\n",
        "    min_count = min(filtered_metadata['country'].value_counts())\n",
        "    print(f\"\\nBalancing datasets to {min_count} samples per country\")\n",
        "\n",
        "    balanced_data = []\n",
        "    for country in top_countries:\n",
        "        country_data = filtered_metadata[filtered_metadata['country'] == country]\n",
        "        # Use sample with index preservation\n",
        "        sampled_data = country_data.sample(n=min_count, random_state=42)\n",
        "        balanced_data.append(sampled_data)\n",
        "\n",
        "    balanced_metadata = pd.concat(balanced_data)\n",
        "\n",
        "    # Create label encoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    country_labels = label_encoder.fit_transform(balanced_metadata['country'])\n",
        "\n",
        "\n",
        "    return balanced_metadata, label_encoder, country_labels.astype(np.int64)\n",
        "\n",
        "def compute_mtl_loss(outputs: Dict[str, torch.Tensor],\n",
        "                    age_batch: torch.Tensor,\n",
        "                    country_batch: torch.Tensor,\n",
        "                    regression_criterion: nn.Module,\n",
        "                    classification_criterion: nn.Module,\n",
        "                    reg_weight: float = 1.0,\n",
        "                    cls_weight: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Compute combined loss for multi-task learning.\"\"\"\n",
        "    regression_loss = regression_criterion(outputs['regression_output'], age_batch)\n",
        "    classification_loss = classification_criterion(outputs['classification_output'], country_batch)\n",
        "\n",
        "    uncertainty_loss = UncertaintyLoss()\n",
        "    total_loss = uncertainty_loss([regression_loss, classification_loss])\n",
        "\n",
        "    return total_loss, regression_loss, classification_loss\n",
        "\n",
        "# Part 3: Visualization Functions\n",
        "def create_confusion_matrix_plot(true_labels: np.ndarray,\n",
        "                               pred_labels: np.ndarray,\n",
        "                               label_encoder: LabelEncoder) -> wandb.Image:\n",
        "    \"\"\"Create and save confusion matrix visualization.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "    # Calculate percentages\n",
        "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "\n",
        "    plt.title('Confusion Matrix for Country Classification')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "\n",
        "    # Customize appearance\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    # Adjust layout to prevent label cutoff\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return wandb.Image('confusion_matrix.png')\n",
        "\n",
        "# Part 3 Continued: Complete Regression Plot Function\n",
        "def create_regression_plot(true_ages: np.ndarray,\n",
        "                         pred_ages: np.ndarray,\n",
        "                         mae: float,\n",
        "                         r2: float,\n",
        "                         mae_std: float = None) -> wandb.Image:\n",
        "    \"\"\"Create and save regression plot.\"\"\"\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    # Scatter plot\n",
        "    plt.scatter(true_ages, pred_ages, alpha=0.3, color='#4169E1',\n",
        "               edgecolor='none', s=60, label='Test Predictions')\n",
        "\n",
        "    # Best fit line\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(true_ages, pred_ages)\n",
        "    line_x = np.linspace(min(true_ages), max(true_ages), 100)\n",
        "    line_y = slope * line_x + intercept\n",
        "    plt.plot(line_x, line_y, color='#C4161C', linestyle='--',\n",
        "             label=f'Best Fit (R² = {r2:.3f})')\n",
        "\n",
        "    # Perfect prediction line\n",
        "    plt.plot([min(true_ages), max(true_ages)],\n",
        "             [min(true_ages), max(true_ages)],\n",
        "             color='black', linestyle='-', alpha=0.3,\n",
        "             label='Perfect Prediction')\n",
        "\n",
        "    # Labels and title\n",
        "    plt.xlabel('True Age (years)', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Predicted Age (years)', fontsize=12, fontweight='bold')\n",
        "    if mae_std is not None:\n",
        "        plt.title(f'MAE = {mae:.2f} ± {mae_std:.2f} years',\n",
        "                 fontsize=14, fontweight='bold', pad=15)\n",
        "    else:\n",
        "        plt.title(f'MAE = {mae:.2f} years',\n",
        "                 fontsize=14, fontweight='bold', pad=15)\n",
        "\n",
        "    # Customize appearance\n",
        "    plt.grid(True, linestyle='--', alpha=0.3)\n",
        "    plt.legend(frameon=True, facecolor='white', framealpha=1,\n",
        "              edgecolor='none', loc='upper left')\n",
        "    plt.axis('equal')\n",
        "\n",
        "    # Customize spines\n",
        "    for spine in plt.gca().spines.values():\n",
        "        spine.set_linewidth(1.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('regression_plot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return wandb.Image('regression_plot.png')\n",
        "\n",
        "def create_training_plots(history: Dict[str, List[float]]) -> Tuple[wandb.Image, wandb.Image]:\n",
        "    \"\"\"Create training history plots.\"\"\"\n",
        "    # Loss plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history['train_reg_loss'], label='Train Regression Loss')\n",
        "    plt.plot(history['val_reg_loss'], label='Val Regression Loss')\n",
        "    plt.plot(history['train_cls_loss'], label='Train Classification Loss')\n",
        "    plt.plot(history['val_cls_loss'], label='Val Classification Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('loss_plot.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Metrics plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history['val_mae'], label='Validation MAE')\n",
        "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.title('Validation Metrics')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('metrics_plot.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    return wandb.Image('loss_plot.png'), wandb.Image('metrics_plot.png')\n",
        "\n",
        "def create_combined_results_figure(confusion_matrix_img: wandb.Image,\n",
        "                                 regression_plot_img: wandb.Image,\n",
        "                                 save_path: str = 'combined_results.png') -> wandb.Image:\n",
        "    \"\"\"Create a combined figure with both confusion matrix and regression plot.\"\"\"\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    # Add confusion matrix\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(plt.imread('confusion_matrix.png'))\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Add regression plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(plt.imread('regression_plot.png'))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return wandb.Image(save_path)\n",
        "\n",
        "# Part 4: Training Functions\n",
        "from typing import Dict, Tuple, List, Any\n",
        "\n",
        "def train_mtl_model(model: nn.Module,\n",
        "                   dataloaders: Tuple[DataLoader, DataLoader, DataLoader],\n",
        "                   criteria: Tuple[nn.Module, nn.Module],\n",
        "                   optimizer: optim.Optimizer,\n",
        "                   run: Any,\n",
        "                   num_epochs: int = 20,\n",
        "                   device: str = 'cuda',\n",
        "                   scaler_y: StandardScaler = None,\n",
        "                   label_encoder: LabelEncoder = None,\n",
        "                   early_stopping_patience: int = 5000) -> Tuple[nn.Module, Dict[str, List[float]], Dict]:\n",
        "    \"\"\"Train the multi-task learning model with proper best model tracking.\"\"\"\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "    regression_criterion, classification_criterion = criteria\n",
        "\n",
        "    # Initialize best model tracking\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "    best_metrics = {}\n",
        "\n",
        "    # Initialize history tracking\n",
        "    history = {\n",
        "        'train_reg_loss': [], 'train_cls_loss': [],\n",
        "        'val_reg_loss': [], 'val_cls_loss': [],\n",
        "        'val_mae': [], 'val_accuracy': []\n",
        "    }\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=100,  # Number of epochs before first restart\n",
        "        T_mult=2,  # Multiply T_0 by this factor after each restart\n",
        "        eta_min=1e-6  # Minimum learning rate\n",
        "    )\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_reg_loss = 0.0\n",
        "        train_cls_loss = 0.0\n",
        "\n",
        "        for x_batch, y_age_batch, y_country_batch in train_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_age_batch = y_age_batch.to(device)\n",
        "            y_country_batch = y_country_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "\n",
        "            total_loss, reg_loss, cls_loss = compute_mtl_loss(\n",
        "                outputs, y_age_batch, y_country_batch,\n",
        "                regression_criterion, classification_criterion\n",
        "            )\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_reg_loss += reg_loss.item()\n",
        "            train_cls_loss += cls_loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_reg_loss = 0.0\n",
        "        val_cls_loss = 0.0\n",
        "        val_preds_age = []\n",
        "        val_true_age = []\n",
        "        val_preds_country = []\n",
        "        val_true_country = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_age_batch, y_country_batch in val_loader:\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_age_batch = y_age_batch.to(device)\n",
        "                y_country_batch = y_country_batch.to(device)\n",
        "\n",
        "                outputs = model(x_batch)\n",
        "\n",
        "                _, reg_loss, cls_loss = compute_mtl_loss(\n",
        "                    outputs, y_age_batch, y_country_batch,\n",
        "                    regression_criterion, classification_criterion\n",
        "                )\n",
        "\n",
        "                val_reg_loss += reg_loss.item()\n",
        "                val_cls_loss += cls_loss.item()\n",
        "\n",
        "                val_preds_age.extend(outputs['regression_output'].cpu().numpy())\n",
        "                val_true_age.extend(y_age_batch.cpu().numpy())\n",
        "                val_preds_country.extend(outputs['classification_output'].argmax(1).cpu().numpy())\n",
        "                val_true_country.extend(y_country_batch.cpu().numpy())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_preds_age = np.array(val_preds_age)\n",
        "        val_true_age = np.array(val_true_age)\n",
        "        if scaler_y is not None:\n",
        "            val_preds_age = scaler_y.inverse_transform(val_preds_age)\n",
        "            val_true_age = scaler_y.inverse_transform(val_true_age)\n",
        "\n",
        "        val_mae = mean_absolute_error(val_true_age, val_preds_age)\n",
        "        val_accuracy = (np.array(val_preds_country) == np.array(val_true_country)).mean()\n",
        "\n",
        "        # Update history\n",
        "        val_total_loss = val_reg_loss + val_cls_loss\n",
        "        history['train_reg_loss'].append(train_reg_loss / len(train_loader))\n",
        "        history['train_cls_loss'].append(train_cls_loss / len(train_loader))\n",
        "        history['val_reg_loss'].append(val_reg_loss / len(val_loader))\n",
        "        history['val_cls_loss'].append(val_cls_loss / len(val_loader))\n",
        "        history['val_mae'].append(val_mae)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "        # Log metrics\n",
        "        run.log({\n",
        "            'epoch': epoch,\n",
        "            'train_regression_loss': history['train_reg_loss'][-1],\n",
        "            'train_classification_loss': history['train_cls_loss'][-1],\n",
        "            'val_regression_loss': history['val_reg_loss'][-1],\n",
        "            'val_classification_loss': history['val_cls_loss'][-1],\n",
        "            'val_mae': val_mae,\n",
        "            'val_accuracy': val_accuracy\n",
        "        })\n",
        "\n",
        "        # Save best model\n",
        "        if val_total_loss < best_val_loss:\n",
        "            best_val_loss = val_total_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Store best metrics\n",
        "            best_metrics = {\n",
        "                'val_mae': val_mae,\n",
        "                'val_accuracy': val_accuracy,\n",
        "                'val_reg_loss': val_reg_loss / len(val_loader),\n",
        "                'val_cls_loss': val_cls_loss / len(val_loader),\n",
        "                'epoch': epoch\n",
        "            }\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    print(f\"\\nLoading best model from epoch {best_epoch}\")\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, history, best_metrics\n",
        "\n",
        "def evaluate_model(model: nn.Module,\n",
        "                  test_loader: DataLoader,\n",
        "                  regression_criterion: nn.Module,\n",
        "                  classification_criterion: nn.Module,\n",
        "                  device: str,\n",
        "                  scaler_y: StandardScaler = None) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate the model on test data using the best validation model.\"\"\"\n",
        "    model.eval()\n",
        "    test_reg_loss = 0.0\n",
        "    test_cls_loss = 0.0\n",
        "    test_preds_age = []\n",
        "    test_true_age = []\n",
        "    test_preds_country = []\n",
        "    test_true_country = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_age_batch, y_country_batch in test_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_age_batch = y_age_batch.to(device)\n",
        "            y_country_batch = y_country_batch.to(device)\n",
        "\n",
        "            outputs = model(x_batch)\n",
        "\n",
        "            _, reg_loss, cls_loss = compute_mtl_loss(\n",
        "                outputs, y_age_batch, y_country_batch,\n",
        "                regression_criterion, classification_criterion\n",
        "            )\n",
        "\n",
        "            test_reg_loss += reg_loss.item()\n",
        "            test_cls_loss += cls_loss.item()\n",
        "\n",
        "            test_preds_age.extend(outputs['regression_output'].cpu().numpy())\n",
        "            test_true_age.extend(y_age_batch.cpu().numpy())\n",
        "            test_preds_country.extend(outputs['classification_output'].argmax(1).cpu().numpy())\n",
        "            test_true_country.extend(y_country_batch.cpu().numpy())\n",
        "\n",
        "    # Calculate test metrics\n",
        "    test_preds_age = np.array(test_preds_age)\n",
        "    test_true_age = np.array(test_true_age)\n",
        "    if scaler_y is not None:\n",
        "        test_preds_age = scaler_y.inverse_transform(test_preds_age)\n",
        "        test_true_age = scaler_y.inverse_transform(test_true_age)\n",
        "\n",
        "    test_mae = mean_absolute_error(test_true_age, test_preds_age)\n",
        "    test_r2 = r2_score(test_true_age, test_preds_age)\n",
        "    test_accuracy = (np.array(test_preds_country) == np.array(test_true_country)).mean()\n",
        "\n",
        "    return {\n",
        "        'test_mae': test_mae,\n",
        "        'test_r2': test_r2,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'test_reg_loss': test_reg_loss / len(test_loader),\n",
        "        'test_cls_loss': test_cls_loss / len(test_loader),\n",
        "        'test_preds_age': test_preds_age,\n",
        "        'test_true_age': test_true_age,\n",
        "        'test_preds_country': test_preds_country,\n",
        "        'test_true_country': test_true_country\n",
        "    }\n",
        "\n",
        "def calculate_overall_metrics(true_ages, pred_ages, true_countries, pred_countries, fold_results):\n",
        "    \"\"\"Calculate overall metrics with standard deviations.\"\"\"\n",
        "    try:\n",
        "        # Ensure arrays are properly shaped\n",
        "        true_ages = true_ages.flatten()\n",
        "        pred_ages = pred_ages.flatten()\n",
        "        true_countries = true_countries.flatten()\n",
        "        pred_countries = pred_countries.flatten()\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        overall_mae = mean_absolute_error(true_ages, pred_ages)\n",
        "        overall_r2 = r2_score(true_ages, pred_ages)\n",
        "        overall_accuracy = np.mean(true_countries == pred_countries)\n",
        "\n",
        "        # Compute standard deviations across folds\n",
        "        mae_std = np.std([fold['test_mae'] for fold in fold_results])\n",
        "        r2_std = np.std([fold['test_r2'] for fold in fold_results])\n",
        "        accuracy_std = np.std([fold['test_accuracy'] for fold in fold_results])\n",
        "\n",
        "        return {\n",
        "            'mae': overall_mae,\n",
        "            'mae_std': mae_std,\n",
        "            'r2': overall_r2,\n",
        "            'r2_std': r2_std,\n",
        "            'accuracy': overall_accuracy,\n",
        "            'accuracy_std': accuracy_std\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in calculate_overall_metrics: {str(e)}\")\n",
        "        print(f\"Original shapes:\")\n",
        "        print(f\"true_ages: {true_ages.shape}, pred_ages: {pred_ages.shape}\")\n",
        "        print(f\"true_countries: {true_countries.shape}, pred_countries: {pred_countries.shape}\")\n",
        "        print(\"Number of folds:\", len(fold_results))\n",
        "        raise\n",
        "\n",
        "def create_confusion_matrix_plot(true_labels: np.ndarray,\n",
        "                               pred_labels: np.ndarray,\n",
        "                               label_encoder: LabelEncoder) -> wandb.Image:\n",
        "    \"\"\"Create and save confusion matrix visualization with larger text.\"\"\"\n",
        "    try:\n",
        "        # Set the global font size\n",
        "        plt.rcParams.update({'font.size': 14})  # Increase base font size\n",
        "\n",
        "        plt.figure(figsize=(12, 10))  # Made figure larger\n",
        "        cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "        # Calculate percentages\n",
        "        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "        # Create heatmap with larger annotations\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=label_encoder.classes_,\n",
        "                   yticklabels=label_encoder.classes_,\n",
        "                   annot_kws={'size': 16})  # Larger numbers in cells\n",
        "\n",
        "        # Set larger fonts for title and labels\n",
        "        plt.title('Confusion Matrix for Country Classification',\n",
        "                 fontsize=20, pad=20)  # Larger title\n",
        "        plt.xlabel('Predicted Labels', fontsize=16, labelpad=10)  # Larger x-label\n",
        "        plt.ylabel('True Labels', fontsize=16, labelpad=10)  # Larger y-label\n",
        "\n",
        "        # Customize appearance with larger tick labels\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=14)  # Larger x-tick labels\n",
        "        plt.yticks(rotation=0, fontsize=14)  # Larger y-tick labels\n",
        "\n",
        "        # Adjust layout to prevent label cutoff\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot with high DPI for clarity\n",
        "        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Reset the global font size to default\n",
        "        plt.rcParams.update({'font.size': plt.rcParamsDefault['font.size']})\n",
        "\n",
        "        return 'confusion_matrix.png'\n",
        "    except Exception as e:\n",
        "        print(f\"Error in create_confusion_matrix_plot: {str(e)}\")\n",
        "        print(f\"Shapes - true_labels: {true_labels.shape}, pred_labels: {pred_labels.shape}\")\n",
        "        raise\n",
        "\n",
        "def create_regression_plot(true_ages: np.ndarray,\n",
        "                         pred_ages: np.ndarray,\n",
        "                         mae: float,\n",
        "                         r2: float,\n",
        "                         mae_std: float = None) -> str:\n",
        "    \"\"\"Create and save regression plot.\"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(8, 8))\n",
        "\n",
        "        # Flatten arrays if they're 2D\n",
        "        true_ages = true_ages.flatten()\n",
        "        pred_ages = pred_ages.flatten()\n",
        "\n",
        "        # Scatter plot\n",
        "        plt.scatter(true_ages, pred_ages, alpha=0.3, color='#4169E1',\n",
        "                   edgecolor='none', s=60, label='Test Predictions')\n",
        "\n",
        "        # Best fit line\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(true_ages, pred_ages)\n",
        "        line_x = np.linspace(min(true_ages), max(true_ages), 100)\n",
        "        line_y = slope * line_x + intercept\n",
        "        plt.plot(line_x, line_y, color='#C4161C', linestyle='--',\n",
        "                 label=f'Best Fit (R² = {r2:.3f})')\n",
        "\n",
        "        # Perfect prediction line\n",
        "        plt.plot([min(true_ages), max(true_ages)],\n",
        "                 [min(true_ages), max(true_ages)],\n",
        "                 color='black', linestyle='-', alpha=0.3,\n",
        "                 label='Perfect Prediction')\n",
        "\n",
        "        # Labels and title\n",
        "        plt.xlabel('True Age (years)', fontsize=12, fontweight='bold')\n",
        "        plt.ylabel('Predicted Age (years)', fontsize=12, fontweight='bold')\n",
        "        if mae_std is not None:\n",
        "            plt.title(f'MAE = {mae:.2f} ± {mae_std:.2f} years',\n",
        "                     fontsize=14, fontweight='bold', pad=15)\n",
        "        else:\n",
        "            plt.title(f'MAE = {mae:.2f} years',\n",
        "                     fontsize=14, fontweight='bold', pad=15)\n",
        "\n",
        "        plt.grid(True, linestyle='--', alpha=0.3)\n",
        "        plt.legend(frameon=True, facecolor='white', framealpha=1,\n",
        "                  edgecolor='none', loc='upper left')\n",
        "        plt.axis('equal')\n",
        "\n",
        "        # Customize spines\n",
        "        for spine in plt.gca().spines.values():\n",
        "            spine.set_linewidth(1.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('regression_plot.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        return 'regression_plot.png'  # Return the path instead of wandb.Image\n",
        "    except Exception as e:\n",
        "        print(f\"Error in create_regression_plot: {str(e)}\")\n",
        "        print(f\"Shapes - true_ages: {true_ages.shape}, pred_ages: {pred_ages.shape}\")\n",
        "        raise\n",
        "\n",
        "def create_combined_results_figure(confusion_matrix_path: str,\n",
        "                                 regression_plot_path: str,\n",
        "                                 save_path: str = 'combined_results.png') -> str:\n",
        "    \"\"\"Create a combined figure with both confusion matrix and regression plot.\"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(16, 8))\n",
        "\n",
        "        # Add confusion matrix\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(plt.imread(confusion_matrix_path))\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Add regression plot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(plt.imread(regression_plot_path))\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        return save_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error in create_combined_results_figure: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def create_and_log_visualizations(results, true_ages, pred_ages, true_countries, pred_countries, label_encoder, run):\n",
        "    \"\"\"Create and log all visualizations.\"\"\"\n",
        "    try:\n",
        "        # Create confusion matrix\n",
        "        cm_path = create_confusion_matrix_plot(\n",
        "            true_countries, pred_countries, label_encoder\n",
        "        )\n",
        "\n",
        "        # Create regression plot\n",
        "        reg_path = create_regression_plot(\n",
        "            true_ages, pred_ages,\n",
        "            results['overall_metrics']['mae'],\n",
        "            results['overall_metrics']['r2'],\n",
        "            results['overall_metrics']['mae_std']\n",
        "        )\n",
        "\n",
        "        # Create combined plot\n",
        "        combined_path = create_combined_results_figure(\n",
        "            cm_path,\n",
        "            reg_path\n",
        "        )\n",
        "\n",
        "        # Log to wandb\n",
        "        run.log({\n",
        "            'confusion_matrix': wandb.Image(cm_path),\n",
        "            'regression_plot': wandb.Image(reg_path),\n",
        "            'combined_results': wandb.Image(combined_path),\n",
        "            **results['overall_metrics']\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error in create_and_log_visualizations: {str(e)}\")\n",
        "        print(f\"Shapes - true_ages: {true_ages.shape}, pred_ages: {pred_ages.shape}\")\n",
        "        print(f\"Shapes - true_countries: {true_countries.shape}, pred_countries: {pred_countries.shape}\")\n",
        "        raise\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "from functools import partial\n",
        "\n",
        "# Modified run_mtl_experiment function with skip capability\n",
        "def run_mtl_experiment(params: Dict, n_splits: int = 5, device: str = 'cuda',\n",
        "                      mae_threshold: float = 20.0, accuracy_threshold: float = 0.5) -> Dict:\n",
        "    \"\"\"Run the complete MTL experiment with proper CV and data splits.\"\"\"\n",
        "    # Set random seeds\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=f\"mtl_WGS_{params['body_site']}_final\",\n",
        "        config=params,\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        print(\"Loading data...\")\n",
        "        table = pd.read_csv('data/control.csv', index_col=0, low_memory=False)\n",
        "        age_metadata = pd.read_csv('data/sampleMetadata.csv', index_col='sample_id', low_memory=False)\n",
        "        age_metadata = age_metadata.loc[(age_metadata.age.notna()) & (age_metadata.body_site == params['body_site'])]\n",
        "        table = table.loc[table.index.isin(age_metadata.index)]\n",
        "        table = table.drop_duplicates(subset='subject_id', keep='first')\n",
        "        shared_index = table.index.intersection(age_metadata.index)\n",
        "        age_metadata = age_metadata.loc[shared_index]\n",
        "\n",
        "        # Get data with stratification labels\n",
        "        X, y_age, y_country, label_encoder, stratify_labels, subject_ids = prepare_mtl_datasets(table, age_metadata, params)\n",
        "        X = matrix_rclr(X)\n",
        "\n",
        "        # Modified cross-validation setup\n",
        "        cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "        # Initialize results structure\n",
        "        results = {\n",
        "            'aborted': False,\n",
        "            'fold_results': [],\n",
        "            'overall_metrics': {},\n",
        "            'best_validation_metrics': [],\n",
        "            'predictions': {\n",
        "                'true_ages': np.zeros(len(X)),\n",
        "                'pred_ages': np.zeros(len(X)),\n",
        "                'true_countries': np.zeros(len(X), dtype=int),\n",
        "                'pred_countries': np.zeros(len(X), dtype=int),\n",
        "                'fold_indices': np.zeros(len(X), dtype=int)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for fold, (train_idx, test_idx) in enumerate(cv.split(X, stratify_labels, groups=subject_ids)):\n",
        "            # [Fold processing remains unchanged until after test_metrics...]\n",
        "            print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
        "            # Create validation split from training data\n",
        "            sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
        "            train_idx_final, val_idx = next(sss.split(X[train_idx], y_country[train_idx]))\n",
        "\n",
        "            # Map indices back to original data\n",
        "            train_idx_final = train_idx[train_idx_final]\n",
        "            val_idx = train_idx[val_idx]\n",
        "\n",
        "            # Split data\n",
        "            X_train = X[train_idx_final]\n",
        "            X_val = X[val_idx]\n",
        "            X_test = X[test_idx]\n",
        "\n",
        "            y_age_train = y_age[train_idx_final]\n",
        "            y_age_val = y_age[val_idx]\n",
        "            y_age_test = y_age[test_idx]\n",
        "\n",
        "            y_country_train = y_country[train_idx_final]\n",
        "            y_country_val = y_country[val_idx]\n",
        "            y_country_test = y_country[test_idx]\n",
        "\n",
        "            # Scale features\n",
        "            scaler_X = StandardScaler()\n",
        "            X_train_scaled = X_train\n",
        "            X_val_scaled = X_val\n",
        "            X_test_scaled = X_test\n",
        "\n",
        "            # Scale age values\n",
        "            scaler_y = MinMaxScaler()\n",
        "            y_age_train_scaled = scaler_y.fit_transform(y_age_train)\n",
        "            y_age_val_scaled = scaler_y.transform(y_age_val)\n",
        "            y_age_test_scaled = scaler_y.transform(y_age_test)\n",
        "\n",
        "            # Create data loaders\n",
        "            train_data = TensorDataset(\n",
        "                torch.FloatTensor(X_train_scaled),\n",
        "                torch.FloatTensor(y_age_train_scaled),\n",
        "                torch.LongTensor(y_country_train)\n",
        "            )\n",
        "            val_data = TensorDataset(\n",
        "                torch.FloatTensor(X_val_scaled),\n",
        "                torch.FloatTensor(y_age_val_scaled),\n",
        "                torch.LongTensor(y_country_val)\n",
        "            )\n",
        "            test_data = TensorDataset(\n",
        "                torch.FloatTensor(X_test_scaled),\n",
        "                torch.FloatTensor(y_age_test_scaled),\n",
        "                torch.LongTensor(y_country_test)\n",
        "            )\n",
        "\n",
        "            train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
        "            val_loader = DataLoader(val_data, batch_size=params['batch_size'])\n",
        "            test_loader = DataLoader(test_data, batch_size=params['batch_size'])\n",
        "\n",
        "            # Initialize model\n",
        "            model = MTLNormalizedTransformer(\n",
        "                input_dim=X.shape[1],\n",
        "                hidden_dim=params['hidden_dim'],\n",
        "                num_layers=params['num_layers'],\n",
        "                num_countries=len(label_encoder.classes_)\n",
        "            ).to(device)\n",
        "\n",
        "            # Train model\n",
        "            optimizer = params['optimizer'](\n",
        "                model.parameters(),\n",
        "                lr=params['learning_rate'],\n",
        "                weight_decay=params['weight_decay']\n",
        "            )\n",
        "            regression_criterion = nn.MSELoss()\n",
        "            classification_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Train with best validation model tracking\n",
        "            model, history, best_val_metrics = train_mtl_model(\n",
        "                model=model,\n",
        "                dataloaders=(train_loader, val_loader, test_loader),\n",
        "                criteria=(regression_criterion, classification_criterion),\n",
        "                optimizer=optimizer,\n",
        "                run=run,\n",
        "                num_epochs=params['num_epochs'],\n",
        "                device=device,\n",
        "                scaler_y=scaler_y,\n",
        "                label_encoder=label_encoder,\n",
        "                early_stopping_patience=5000,\n",
        "            )\n",
        "\n",
        "            # Evaluate using best validation model\n",
        "            test_metrics = evaluate_model(\n",
        "                model=model,\n",
        "                test_loader=test_loader,\n",
        "                regression_criterion=regression_criterion,\n",
        "                classification_criterion=classification_criterion,\n",
        "                device=device,\n",
        "                scaler_y=scaler_y\n",
        "            )\n",
        "            # Store results\n",
        "            fold_results = {\n",
        "                'fold': fold + 1,\n",
        "                'test_mae': test_metrics['test_mae'],\n",
        "                'test_r2': test_metrics['test_r2'],\n",
        "                'test_accuracy': test_metrics['test_accuracy'],\n",
        "                'best_validation_epoch': best_val_metrics['epoch'],\n",
        "                'best_validation_mae': best_val_metrics['val_mae'],\n",
        "                'best_validation_accuracy': best_val_metrics['val_accuracy'],\n",
        "                'test_indices': test_idx\n",
        "            }\n",
        "\n",
        "            results['fold_results'].append(fold_results)\n",
        "            results['best_validation_metrics'].append(best_val_metrics)\n",
        "\n",
        "            # Check if current fold exceeds thresholds\n",
        "            if (test_metrics['test_mae'] > mae_threshold or\n",
        "                test_metrics['test_accuracy'] < accuracy_threshold):\n",
        "                print(f\"Fold {fold+1} failed thresholds (MAE > {mae_threshold} or \"\n",
        "                      f\"Accuracy < {accuracy_threshold}). Aborting trial.\")\n",
        "                results['aborted'] = True\n",
        "                break  # Exit fold loop early\n",
        "\n",
        "            # Store predictions in their correct positions\n",
        "            results['predictions']['true_ages'][test_idx] = test_metrics['test_true_age'].flatten()\n",
        "            results['predictions']['pred_ages'][test_idx] = test_metrics['test_preds_age'].flatten()\n",
        "            results['predictions']['true_countries'][test_idx] = test_metrics['test_true_country']\n",
        "            results['predictions']['pred_countries'][test_idx] = test_metrics['test_preds_country']\n",
        "            results['predictions']['fold_indices'][test_idx] = fold + 1\n",
        "\n",
        "            # Log fold results\n",
        "            run.log({\n",
        "                f'fold_{fold+1}_test_mae': test_metrics['test_mae'],\n",
        "                f'fold_{fold+1}_test_r2': test_metrics['test_r2'],\n",
        "                f'fold_{fold+1}_test_accuracy': test_metrics['test_accuracy']\n",
        "            })\n",
        "\n",
        "            print(f\"\\nFold {fold + 1} Results:\")\n",
        "            print(f\"Test MAE: {test_metrics['test_mae']:.3f}\")\n",
        "            print(f\"Test R2: {test_metrics['test_r2']:.3f}\")\n",
        "            print(f\"Test Accuracy: {test_metrics['test_accuracy']:.3f}\")\n",
        "\n",
        "        print(\"\\nCalculating overall metrics...\")\n",
        "        print(f\"Number of samples: {len(X)}\")\n",
        "        print(f\"Number of folds: {len(results['fold_results'])}\")\n",
        "\n",
        "        # Verify each sample was used exactly once\n",
        "        fold_counts = np.bincount(results['predictions']['fold_indices'].astype(int))[1:]\n",
        "        print(\"\\nSamples per fold:\", fold_counts)\n",
        "\n",
        "        # Only calculate metrics if trial wasn't aborted\n",
        "        if not results['aborted']:\n",
        "            try:\n",
        "                # Calculate overall metrics\n",
        "                results['overall_metrics'] = calculate_overall_metrics(\n",
        "                    results['predictions']['true_ages'],\n",
        "                    results['predictions']['pred_ages'],\n",
        "                    results['predictions']['true_countries'],\n",
        "                    results['predictions']['pred_countries'],\n",
        "                    results['fold_results']\n",
        "                )\n",
        "\n",
        "                print(\"\\nCreating visualizations...\")\n",
        "                # Create visualizations\n",
        "                create_and_log_visualizations(\n",
        "                    results,\n",
        "                    results['predictions']['true_ages'],\n",
        "                    results['predictions']['pred_ages'],\n",
        "                    results['predictions']['true_countries'],\n",
        "                    results['predictions']['pred_countries'],\n",
        "                    label_encoder,\n",
        "                    run\n",
        "                )\n",
        "\n",
        "                # Print final results\n",
        "                print(\"\\nFinal Results:\")\n",
        "                print(f\"MAE: {results['overall_metrics']['mae']:.2f} ± {results['overall_metrics']['mae_std']:.2f}\")\n",
        "                print(f\"R²: {results['overall_metrics']['r2']:.3f} ± {results['overall_metrics']['r2_std']:.3f}\")\n",
        "                print(f\"Accuracy: {results['overall_metrics']['accuracy']:.3f} ± {results['overall_metrics']['accuracy_std']:.3f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in metrics calculation: {str(e)}\")\n",
        "                print(\"Prediction shapes:\")\n",
        "                for key, value in results['predictions'].items():\n",
        "                    print(f\"{key}: {value.shape}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(\"Trial aborted due to failed thresholds. Skipping final metrics.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Experiment failed: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        run.log({'error': error_msg})\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        run.finish()\n",
        "        import gc\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    return results\n",
        "\n",
        "# Modified objective function\n",
        "def objective(trial: optuna.Trial, device: str = 'cuda') -> float:\n",
        "    \"\"\"Objective function with early trial termination.\"\"\"\n",
        "    params = {\n",
        "        'body_site': 'stool',\n",
        "        'hidden_dim': trial.suggest_categorical('hidden_dim', [256]),\n",
        "        'num_layers': 1,\n",
        "        'batch_size': trial.suggest_categorical('batch_size', [4096]),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 7e-1, log=True),\n",
        "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1, log=True),\n",
        "        'test_split': 0.2,\n",
        "        'num_epochs': trial.suggest_categorical('num_epochs', [1000]),\n",
        "        'normalize_X': False,\n",
        "        'normalize_y': True,\n",
        "    }\n",
        "\n",
        "    # Optimizer selection\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['SGD'])#\n",
        "    params['optimizer'] = optim.AdamW if optimizer_name == 'AdamW' else optim.SGD\n",
        "\n",
        "\n",
        "    try:\n",
        "        results = run_mtl_experiment(\n",
        "            params,\n",
        "            n_splits=10,\n",
        "            device=device,\n",
        "            mae_threshold=12.0,      # Set your MAE threshold\n",
        "            accuracy_threshold=0.75   # Set your accuracy threshold\n",
        "        )\n",
        "\n",
        "        if results['aborted']:\n",
        "            # Return large value to mark bad trial\n",
        "            trial.set_user_attr('aborted_reason', 'Threshold exceeded')\n",
        "            return 1000.0  # High value for minimization\n",
        "\n",
        "        # Calculate combined score (lower is better)\n",
        "        combined_score = results['overall_metrics']['mae'] * (1 - results['overall_metrics']['accuracy'])\n",
        "        # Log additional metrics to Optuna\n",
        "        trial.set_user_attr('accuracy', results['overall_metrics']['accuracy'])\n",
        "        trial.set_user_attr('mae', results['overall_metrics']['mae'])\n",
        "        trial.set_user_attr('r2', results['overall_metrics']['r2'])\n",
        "\n",
        "        return combined_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Trial failed: {str(e)}\")\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "\n",
        "def run_optimization(n_trials: int = 200, device: str = 'cuda') -> optuna.Study:\n",
        "    \"\"\"Run hyperparameter optimization.\"\"\"\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        study_name=\"microbiome_mtl_optimization\",\n",
        "        sampler=optuna.samplers.TPESampler(seed=42),\n",
        "        pruner=optuna.pruners.MedianPruner(\n",
        "            n_startup_trials=10,\n",
        "            n_warmup_steps=40,\n",
        "            interval_steps=20\n",
        "        )\n",
        "    )\n",
        "\n",
        "    study.optimize(\n",
        "        partial(objective, device=device),\n",
        "        n_trials=n_trials,\n",
        "        callbacks=[\n",
        "            lambda study, trial: print(f\"\\nTrial {trial.number} finished with score: {trial.value}\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Print optimization results\n",
        "    print(\"\\nStudy statistics: \")\n",
        "    print(f\"  Number of finished trials: {len(study.trials)}\")\n",
        "    print(f\"  Number of pruned trials: {len(study.get_trials(states=[TrialState.PRUNED]))}\")\n",
        "    print(f\"  Number of complete trials: {len(study.get_trials(states=[TrialState.COMPLETE]))}\")\n",
        "\n",
        "    print(\"\\nBest trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"  Best combined score: {trial.value}\")\n",
        "    print(\"  Best parameters:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n",
        "    # Calculate parameter importances\n",
        "    try:\n",
        "        importances = optuna.importance.get_param_importances(study)\n",
        "        print(\"\\nParameter importances:\")\n",
        "        for param, importance in importances.items():\n",
        "            print(f\"    {param}: {importance:.3f}\")\n",
        "    except:\n",
        "        print(\"Could not compute parameter importances\")\n",
        "\n",
        "    return study\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Run optimization\n",
        "    study = run_optimization(n_trials=3, device=device)\n",
        "\n",
        "    # Save study results\n",
        "    study.trials_dataframe().to_csv(\"optuna_results.csv\")\n",
        "\n",
        "    # Get best parameters\n",
        "    best_params = study.best_trial.params\n",
        "    final_params = {\n",
        "        'body_site': 'stool',\n",
        "        'hidden_dim': best_params['hidden_dim'],\n",
        "        'num_layers': best_params['num_layers'],\n",
        "        'batch_size': best_params['batch_size'],\n",
        "        'learning_rate': best_params['learning_rate'],\n",
        "        'weight_decay': best_params['weight_decay'],\n",
        "        'test_split': 0.2,\n",
        "        'num_epochs': 1000,\n",
        "        'normalize_X': False,\n",
        "        'normalize_y': True,\n",
        "        'optimizer': optim.AdamW if best_params['optimizer'] == 'AdamW' else optim.SGD\n",
        "    }\n",
        "\n",
        "    # Run final experiment with best parameters\n",
        "    print(\"\\nRunning final experiment with best parameters...\")\n",
        "    final_results = run_mtl_experiment(final_params, n_splits=5, device=device)\n",
        "\n",
        "    # Save final results\n",
        "    with open('final_results.json', 'w') as f:\n",
        "        json.dump({\n",
        "            'best_parameters': best_params,\n",
        "            'final_metrics': final_results['overall_metrics']\n",
        "        }, f, indent=2)\n",
        "\n",
        "    print(\"\\nResults have been saved to 'final_results.json'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "361d74fac40741968ff1c2f9bb47b151": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e01fd437b2541899bc494962ac8d5a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d752b2e912340648faac4320a1fad2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "740e35c7cf814a1587a91c49a840b8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d752b2e912340648faac4320a1fad2a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_361d74fac40741968ff1c2f9bb47b151",
            "value": 1
          }
        },
        "7423c0986ab64d608d46e6f89bbfccbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74d9e5e6d87408c85f09eecac379f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e01fd437b2541899bc494962ac8d5a5",
            "placeholder": "​",
            "style": "IPY_MODEL_c4376bac09d645ee86398874967a7228",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "c4376bac09d645ee86398874967a7228": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d314bbdf6b76483984fdcf450b8de222": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a74d9e5e6d87408c85f09eecac379f08",
              "IPY_MODEL_740e35c7cf814a1587a91c49a840b8c8"
            ],
            "layout": "IPY_MODEL_7423c0986ab64d608d46e6f89bbfccbd"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
