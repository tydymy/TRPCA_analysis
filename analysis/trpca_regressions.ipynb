{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T46cz1GE4wjl"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, StratifiedGroupKFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import wandb\n",
        "from itertools import product\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from gemelli.preprocessing import matrix_rclr\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import math\n",
        "from functools import partial\n",
        "from biom import load_table\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDOYiuZBKwsg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "### version where pc vectors are projected into multiple views in a higher dim space.\n",
        "class NormalizedTransformerBlock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(NormalizedTransformerBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=4, dropout=0, batch_first=True)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "        )\n",
        "        self.alphaA = nn.Parameter(torch.tensor(1.0))  # Learnable scaling for attention updates\n",
        "        self.alphaM = nn.Parameter(torch.tensor(1.0))  # Learnable scaling for MLP updates\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Normalize input\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "        # Attention block\n",
        "        hA, _ = self.attention(x, x, x)\n",
        "        hA = F.normalize(hA, p=2, dim=-1)\n",
        "        x = F.normalize(x + self.alphaA * (hA - x), p=2, dim=-1)\n",
        "\n",
        "        # MLP block\n",
        "        hM = self.mlp(x)\n",
        "        hM = F.normalize(hM, p=2, dim=-1)\n",
        "        x = F.normalize(x + self.alphaM * (hM - x), p=2, dim=-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class NormalizedTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, projection_dim=4):\n",
        "        super(NormalizedTransformer, self).__init__()\n",
        "        self.projection_dim = projection_dim\n",
        "\n",
        "        # Project PCA vector to hidden_dim\n",
        "        self.pca_projection = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Generate different \"views\" of the projected PCA vector\n",
        "        self.view_generator = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, projection_dim * hidden_dim),\n",
        "            nn.LayerNorm(projection_dim * hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Transformer blocks remain the same\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [NormalizedTransformerBlock(hidden_dim, hidden_dim * 2) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.regression_head = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch, pca_dim]\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Project PCA vector to hidden dimension\n",
        "        x = self.pca_projection(x)  # Shape: [batch, hidden_dim]\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "        # Generate multiple views of the projected vector\n",
        "        x = self.view_generator(x)  # Shape: [batch, projection_dim * hidden_dim]\n",
        "\n",
        "        # Reshape to [batch, projection_dim, hidden_dim]\n",
        "        x = x.view(batch_size, self.projection_dim, -1)\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Global average pooling over projection dimensions\n",
        "        x = x.mean(dim=1)  # Shape: [batch, hidden_dim]\n",
        "\n",
        "        # Regression head\n",
        "        output = self.regression_head(x)\n",
        "        outputs = {'regression_output': output}\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QCELndW4xqk"
      },
      "outputs": [],
      "source": [
        "def calculate_sparsity(model, threshold=1e-5):\n",
        "    \"\"\"\n",
        "    Calculate model sparsity using the formula:\n",
        "    S = (1/D) * sum_{i=1}^D (1/n * sum_{j=1}^n I(a_{i,j} < τ))\n",
        "\n",
        "    where:\n",
        "    - D is the number of layers\n",
        "    - n is the number of parameters in each layer\n",
        "    - a_{i,j} is the j-th parameter in the i-th layer\n",
        "    - τ (tau) is the threshold below which parameters are considered sparse\n",
        "    - I() is the indicator function\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        threshold: float, threshold below which parameters are considered sparse\n",
        "\n",
        "    Returns:\n",
        "        float: sparsity score between 0 and 1\n",
        "    \"\"\"\n",
        "\n",
        "    def count_sparse_elements(tensor, threshold):\n",
        "        \"\"\"Helper function to count elements below threshold\"\"\"\n",
        "        return (torch.abs(tensor) < threshold).float().mean().item()\n",
        "\n",
        "    # Get all parameter tensors\n",
        "    params = list(model.parameters())\n",
        "\n",
        "    # Calculate sparsity for each layer\n",
        "    layer_sparsities = []\n",
        "    for param in params:\n",
        "        if param.dim() > 0:  # Skip scalar parameters\n",
        "            sparsity = count_sparse_elements(param, threshold)\n",
        "            layer_sparsities.append(sparsity)\n",
        "\n",
        "    # Calculate average sparsity across all layers\n",
        "    if layer_sparsities:\n",
        "        total_sparsity = sum(layer_sparsities) / len(layer_sparsities)\n",
        "        return total_sparsity\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "def calculate_weight_entropy(model, epsilon=1e-10):\n",
        "    \"\"\"\n",
        "    Calculate the absolute weight entropy using the formula:\n",
        "    H(W) = -sum_{i=1}^m sum_{j=1}^n |w_{ij}| log|w_{ij}|\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        epsilon: small constant to avoid log(0)\n",
        "\n",
        "    Returns:\n",
        "        float: total weight entropy\n",
        "        dict: layer-wise entropies\n",
        "    \"\"\"\n",
        "    def compute_entropy(tensor):\n",
        "        \"\"\"Helper function to compute entropy for a single tensor\"\"\"\n",
        "        # Flatten the tensor and take absolute values\n",
        "        abs_weights = torch.abs(tensor.flatten())\n",
        "\n",
        "        # Normalize weights to sum to 1 (treating them as probabilities)\n",
        "        normalized_weights = abs_weights / (torch.sum(abs_weights) + epsilon)\n",
        "\n",
        "        # Calculate entropy\n",
        "        entropy = -torch.sum(\n",
        "            normalized_weights * torch.log(normalized_weights + epsilon)\n",
        "        ).item()\n",
        "\n",
        "        return entropy\n",
        "\n",
        "    total_entropy = 0.0\n",
        "    layer_entropies = {}\n",
        "\n",
        "    # Calculate entropy for each layer\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 0:  # Skip scalar parameters\n",
        "            layer_entropy = compute_entropy(param)\n",
        "            layer_entropies[name] = layer_entropy\n",
        "            total_entropy += layer_entropy\n",
        "\n",
        "    return total_entropy, layer_entropies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9rHhVqd4zYR"
      },
      "outputs": [],
      "source": [
        "def train_with_test_loss(model, dataloaders, criterion, optimizer, run, num_epochs=20, device='cuda', scaler_y=None):\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize the cosine annealing scheduler with warm restarts\n",
        "    # T_0 is the number of epochs before first restart\n",
        "    # T_mult is the factor by which T_i increases after each restart\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=500,  # First restart occurs after 100 epochs\n",
        "        T_mult=1,  # Each restart interval is twice as long as the previous one\n",
        "        eta_min=0.0005\n",
        "    )\n",
        "\n",
        "    best_val_mae = float('inf')\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs['regression_output'], y_batch) \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler after each batch\n",
        "        scheduler.step()\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "        # Validation Phase\n",
        "        val_loss = 0.0\n",
        "        y_true_val = []\n",
        "        y_pred_val = []\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(x_batch)\n",
        "                loss = criterion(outputs['regression_output'], y_batch) \n",
        "                val_loss += loss.item()\n",
        "\n",
        "                y_true_val.append(y_batch.cpu().numpy())\n",
        "                y_pred_val.append(outputs['regression_output'].cpu().numpy())\n",
        "\n",
        "            # Test Phase (Monitoring Test Dataset)\n",
        "            test_loss = 0.0\n",
        "            y_true_test = []\n",
        "            y_pred_test = []\n",
        "            for x_batch, y_batch in test_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(x_batch)\n",
        "                loss = criterion(outputs['regression_output'], y_batch)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                y_true_test.append(y_batch.cpu().numpy())\n",
        "                y_pred_test.append(outputs['regression_output'].cpu().numpy())\n",
        "\n",
        "        y_true_val = np.concatenate(y_true_val)\n",
        "        y_pred_val = np.concatenate(y_pred_val)\n",
        "        y_true_test = np.concatenate(y_true_test)\n",
        "        y_pred_test = np.concatenate(y_pred_test)\n",
        "\n",
        "        if scaler_y is not None:\n",
        "            y_true_val_original = scaler_y.inverse_transform(y_true_val)\n",
        "            y_pred_val_original = scaler_y.inverse_transform(y_pred_val)\n",
        "            y_true_test_original = scaler_y.inverse_transform(y_true_test)\n",
        "            y_pred_test_original = scaler_y.inverse_transform(y_pred_test)\n",
        "        else:\n",
        "            y_true_val_original = y_true_val\n",
        "            y_pred_val_original = y_pred_val\n",
        "            y_true_test_original = y_true_test\n",
        "            y_pred_test_original = y_pred_test\n",
        "\n",
        "        val_mae = mean_absolute_error(y_true_val_original, y_pred_val_original)\n",
        "        val_r2 = r2_score(y_true_val_original, y_pred_val_original)\n",
        "        test_mae = mean_absolute_error(y_true_test_original, y_pred_test_original)\n",
        "        test_r2 = r2_score(y_true_test_original, y_pred_test_original)\n",
        "\n",
        "        if val_mae < best_val_mae:\n",
        "            best_val_mae = val_mae\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        test_loss /= len(test_loader)\n",
        "\n",
        "        # Calculate sparsity and entropy metrics\n",
        "        sparsity = calculate_sparsity(model)\n",
        "        abs_weight_entropy, layer_entropies = calculate_weight_entropy(model)\n",
        "\n",
        "        # Log all metrics, including test metrics and learning rate\n",
        "        run.log({\n",
        "            'epoch': epoch,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'test_loss': test_loss,\n",
        "            'val_mae_original_scale': val_mae,\n",
        "            'val_r2_original_scale': val_r2,\n",
        "            'test_mae_original_scale': test_mae,\n",
        "            'test_r2_original_scale': test_r2,\n",
        "            'sparsity': sparsity,\n",
        "            'absolute_weight_entropy': abs_weight_entropy,\n",
        "            'learning_rate': current_lr\n",
        "        })\n",
        "\n",
        "    # After training, load the best model state\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frPrDjjTYgq4"
      },
      "outputs": [],
      "source": [
        "def run_cv_experiment(params, n_splits=5, device='cuda'):\n",
        "    \"\"\"\n",
        "    Run cross-validation experiment with transformer model and save indexed predictions.\n",
        "\n",
        "    Args:\n",
        "        params (dict): Model and training parameters\n",
        "        n_splits (int): Number of CV splits\n",
        "        device (str): Computing device ('cuda' or 'cpu')\n",
        "    \"\"\"\n",
        "    # Initialize wandb\n",
        "    run = wandb.init(\n",
        "        project=f\"wgs_single_reviewer_{params['body_site']}\",\n",
        "        config=params,\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Data preparation for WGS\n",
        "        table = pd.read_csv('control.csv', index_col=0)\n",
        "        age_metadata = pd.read_csv('sampleMetadata.csv', index_col='sample_id', dtype={'age': float})\n",
        "        age_metadata = age_metadata.loc[(age_metadata.age.notna()) & (age_metadata.body_site == params['body_site'])]\n",
        "        table = table.loc[table.index.isin(age_metadata.index)]\n",
        "        table = table.drop_duplicates(subset='subject_id', keep='first')\n",
        "        shared_index = table.index.intersection(age_metadata.index)\n",
        "        table = table.loc[shared_index].drop(columns=['study_name', 'study_condition', 'subject_id'])\n",
        "        age_metadata = age_metadata.loc[shared_index]\n",
        "\n",
        "        # Remove columns with all zeros\n",
        "        all_zero_columns = (table == 0).all(axis=0)\n",
        "        table = table.loc[:, ~all_zero_columns]\n",
        "        df = (table * 1e7).round().astype(int)\n",
        "\n",
        "        # Data Preparation for 16S\n",
        "        # table = load_table('data/skin_1975.biom').to_dataframe(dense=True).T.astype(int)\n",
        "        # age_metadata = pd.read_csv('data/skin_1975_map.txt', sep='\\t', index_col=0, dtype={'qiita_host_age': float})\n",
        "        # # age_metadata = age_metadata.drop_duplicates(subset='host_subject_id')\n",
        "        # table = table.loc[age_metadata.index]\n",
        "        # columns_to_drop = table.columns[table.apply(lambda col: (col != 0).sum()) < 25]# drop columns with low prev\n",
        "        # df = table.drop(columns=columns_to_drop).copy()\n",
        "        # print(df.shape)\n",
        "        # Prepare target variable\n",
        "        y = age_metadata.age.values.reshape(-1, 1)\n",
        "        arr = np.nan_to_num(matrix_rclr(df.values), nan=0.0)\n",
        "        arr_reduced = np.nan_to_num(matrix_rclr(df.values), nan=0.0)\n",
        "        if arr.ndim > 2:\n",
        "            arr = arr.reshape(arr.shape[0], -1)\n",
        "\n",
        "        # PCA reduction\n",
        "        pca = PCA(n_components=256)\n",
        "        arr_reduced = pca.fit_transform(arr)\n",
        "\n",
        "        print(f\"Original dimensions: {arr.shape}\")\n",
        "        print(f\"Reduced dimensions: {arr_reduced.shape}\")\n",
        "        print(f\"Number of components: {pca.n_components_}\")\n",
        "        print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "        X = torch.tensor(arr_reduced).float()\n",
        "        y = torch.tensor(y).float()\n",
        "\n",
        "        # Initialize scalers\n",
        "        scaler_X = StandardScaler() if params.get('normalize_X', True) else None\n",
        "        scaler_y = MinMaxScaler() if params.get('normalize_y', True) else None\n",
        "\n",
        "        # Apply normalization\n",
        "        X_np = scaler_X.fit_transform(X.numpy()) if scaler_X else X.numpy()\n",
        "        y_np = scaler_y.fit_transform(y.numpy()) if scaler_y else y.numpy()\n",
        "\n",
        "        # Create stratification bins\n",
        "        n_bins = 5\n",
        "        strata = pd.qcut(age_metadata.age, q=n_bins, labels=[f'age_bin_{i}' for i in range(n_bins)]).astype(str)+age_metadata.study_name.astype(str)+age_metadata.country.astype(str)\n",
        "        groups = age_metadata.subject_id.astype(str)\n",
        "\n",
        "\n",
        "        # Filter out samples from strata with less than 10 occurrences\n",
        "        strata_counts = strata.value_counts()\n",
        "        valid_strata = strata_counts[strata_counts >= 10].index\n",
        "\n",
        "        # Create mask and apply to all relevant variables\n",
        "        mask = strata.isin(valid_strata)\n",
        "        X_filtered = X[mask]\n",
        "        y_filtered = y[mask]\n",
        "        X_np = X_np[mask]\n",
        "        y_np = y_np[mask]\n",
        "        age_metadata_filtered = age_metadata[mask]\n",
        "        strata_filtered = strata[mask]\n",
        "        groups_filtered = groups[mask]\n",
        "\n",
        "        # Print filtering stats\n",
        "        print(f\"Original samples: {len(strata)}\")\n",
        "        print(f\"Samples after filtering strata with <10 occurrences: {len(strata_filtered)}\")\n",
        "        print(f\"Removed {len(strata) - len(strata_filtered)} samples\")\n",
        "\n",
        "        # Update variables to use filtered versions\n",
        "        X = X_filtered\n",
        "        y = y_filtered\n",
        "        age_metadata = age_metadata_filtered\n",
        "        strata = strata_filtered\n",
        "        groups = groups_filtered\n",
        "\n",
        "        kf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "        # Initialize prediction tracking\n",
        "        predictions_dict = {\n",
        "            'sample_id': [],\n",
        "            'true_age': [],\n",
        "            'predicted_age': [],\n",
        "            'fold': []\n",
        "        }\n",
        "\n",
        "        fold_results = []\n",
        "\n",
        "        for fold, (train_index, test_index) in enumerate(kf.split(X_np, groups=groups, y=strata), 1):\n",
        "            try:\n",
        "                # Split data for current fold\n",
        "                X_train_full, X_test = X_np[train_index], X_np[test_index]\n",
        "                y_train_full, y_test = y_np[train_index], y_np[test_index]\n",
        "                strata_train_full = strata.iloc[train_index]\n",
        "\n",
        "                # Create validation split\n",
        "                sss = StratifiedShuffleSplit(n_splits=1, test_size=params['test_split'], random_state=42)\n",
        "                train_index_sub, val_index = next(sss.split(X_train_full, y=strata_train_full))\n",
        "\n",
        "                # Final train/val split\n",
        "                X_train = X_train_full[train_index_sub]\n",
        "                y_train = y_train_full[train_index_sub]\n",
        "                X_val = X_train_full[val_index]\n",
        "                y_val = y_train_full[val_index]\n",
        "\n",
        "                # Create data loaders\n",
        "                train_data = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).float())\n",
        "                val_data = TensorDataset(torch.tensor(X_val).float(), torch.tensor(y_val).float())\n",
        "                test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
        "\n",
        "                train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
        "                val_loader = DataLoader(val_data, batch_size=params['batch_size'])\n",
        "                test_loader = DataLoader(test_data, batch_size=params['batch_size'])\n",
        "\n",
        "                model = NormalizedTransformer(\n",
        "                    input_dim=X_train.shape[1],\n",
        "                    num_layers=params['num_layers'],\n",
        "                    hidden_dim=params['hidden_dim'],\n",
        "                    output_dim=1,\n",
        "                ).to(device)\n",
        "\n",
        "                # Initialize weights\n",
        "                def init_weights(m):\n",
        "                    if isinstance(m, nn.Linear):\n",
        "                        torch.nn.init.xavier_uniform_(m.weight)\n",
        "                        if m.bias is not None:\n",
        "                            torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "                model.apply(init_weights)\n",
        "                model = model.to(device)\n",
        "\n",
        "                # Setup optimizer\n",
        "                optimizer = params['optimizer'](\n",
        "                    model.parameters(),\n",
        "                    lr=params['learning_rate'],\n",
        "                    weight_decay=params['weight_decay']\n",
        "                )\n",
        "\n",
        "                criterion = nn.MSELoss()#nn.HuberLoss(delta=10)#\n",
        "\n",
        "                # Training phase\n",
        "                try:\n",
        "                    train_with_test_loss(\n",
        "                        model,\n",
        "                        (train_loader, val_loader, test_loader),\n",
        "                        criterion,\n",
        "                        optimizer,\n",
        "                        run,\n",
        "                        num_epochs=params['num_epochs'],\n",
        "                        device=device,\n",
        "                        scaler_y=scaler_y\n",
        "                    )\n",
        "                except RuntimeError as e:\n",
        "                    if \"nan\" in str(e).lower():\n",
        "                        run.log({\n",
        "                            f'fold_{fold}_error': f'NaN loss detected during training: {str(e)}',\n",
        "                            f'fold_{fold}_status': 'failed_nan_loss'\n",
        "                        })\n",
        "                        print(f\"Fold {fold} failed due to NaN loss. Skipping to next fold.\")\n",
        "                        continue\n",
        "\n",
        "                # Evaluation phase\n",
        "                try:\n",
        "                    model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        batch_start = 0\n",
        "                        for x_batch, y_batch in test_loader:\n",
        "                            # Get indices for current batch\n",
        "                            batch_size = len(x_batch)\n",
        "                            batch_indices = test_index[batch_start:batch_start + batch_size]\n",
        "                            batch_start += batch_size\n",
        "\n",
        "                            x_batch = x_batch.to(device)\n",
        "                            reg_outputs = model(x_batch)['regression_output'].cpu().numpy()\n",
        "\n",
        "                            if np.any(np.isnan(reg_outputs)):\n",
        "                                raise RuntimeError(\"NaN values detected in model predictions\")\n",
        "\n",
        "                            # Convert predictions back to original scale\n",
        "                            y_true_batch = scaler_y.inverse_transform(y_batch.numpy()) if scaler_y else y_batch.numpy()\n",
        "                            y_pred_batch = scaler_y.inverse_transform(reg_outputs) if scaler_y else reg_outputs\n",
        "\n",
        "                            # Store predictions with corresponding indices\n",
        "                            predictions_dict['sample_id'].extend(age_metadata.index[batch_indices])\n",
        "                            predictions_dict['true_age'].extend(y_true_batch.flatten())\n",
        "                            predictions_dict['predicted_age'].extend(y_pred_batch.flatten())\n",
        "                            predictions_dict['fold'].extend([fold] * batch_size)\n",
        "\n",
        "                    # Calculate metrics for this fold\n",
        "                    fold_true = predictions_dict['true_age'][-len(test_index):]\n",
        "                    fold_pred = predictions_dict['predicted_age'][-len(test_index):]\n",
        "\n",
        "                    mae = mean_absolute_error(fold_true, fold_pred)\n",
        "                    r2 = r2_score(fold_true, fold_pred)\n",
        "\n",
        "                    # Log results\n",
        "                    fold_results.append({\n",
        "                        'fold': fold,\n",
        "                        'mae': mae,\n",
        "                        'r2': r2\n",
        "                    })\n",
        "\n",
        "                    run.log({\n",
        "                        f'fold_{fold}_mae': mae,\n",
        "                        f'fold_{fold}_r2': r2,\n",
        "                        f'fold_{fold}_status': 'completed'\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    run.log({\n",
        "                        f'fold_{fold}_error': f'Error during evaluation: {str(e)}',\n",
        "                        f'fold_{fold}_status': 'failed_evaluation'\n",
        "                    })\n",
        "                    print(f\"Error during evaluation of fold {fold}: {str(e)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                run.log({\n",
        "                    f'fold_{fold}_error': f'Fold processing error: {str(e)}',\n",
        "                    f'fold_{fold}_status': 'failed_processing'\n",
        "                })\n",
        "                print(f\"Error processing fold {fold}: {str(e)}\")\n",
        "\n",
        "        # Calculate and log overall metrics\n",
        "        if fold_results:\n",
        "            overall_mae = np.mean([r['mae'] for r in fold_results])\n",
        "            overall_r2 = np.mean([r['r2'] for r in fold_results])\n",
        "            mae_std = np.std([r['mae'] for r in fold_results])\n",
        "            r2_std = np.std([r['r2'] for r in fold_results])\n",
        "\n",
        "            # Create predictions DataFrame\n",
        "            predictions_df = pd.DataFrame(predictions_dict)\n",
        "\n",
        "            # Save predictions locally and to wandb\n",
        "            predictions_df.to_csv('predictions.csv')\n",
        "            table = wandb.Table(dataframe=predictions_df)\n",
        "            run.log({\n",
        "                \"predictions_table\": table,\n",
        "                'overall_mae': overall_mae,\n",
        "                'overall_r2': overall_r2,\n",
        "                'mae_std': mae_std,\n",
        "                'r2_std': r2_std\n",
        "            })\n",
        "\n",
        "            # Create final publication-quality regression plot\n",
        "            plt.figure(figsize=(8, 8))\n",
        "\n",
        "            # Create scatter plot\n",
        "            plt.scatter(predictions_df['true_age'], predictions_df['predicted_age'],\n",
        "                       alpha=0.3, color='#4169E1',\n",
        "                       edgecolor='none', s=60, label='Test Predictions')\n",
        "\n",
        "            # Calculate and plot best fit line\n",
        "            slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
        "                predictions_df['true_age'],\n",
        "                predictions_df['predicted_age']\n",
        "            )\n",
        "            line_x = np.linspace(min(predictions_df['true_age']),\n",
        "                                max(predictions_df['true_age']), 100)\n",
        "            line_y = slope * line_x + intercept\n",
        "            plt.plot(line_x, line_y, color='#C4161C', linestyle='--',\n",
        "                     label=f'Best Fit (R² = {r_value**2:.3f})')\n",
        "\n",
        "            # Add perfect prediction line (y=x)\n",
        "            plt.plot([min(predictions_df['true_age']), max(predictions_df['true_age'])],\n",
        "                     [min(predictions_df['true_age']), max(predictions_df['true_age'])],\n",
        "                     color='black', linestyle='-', alpha=0.3, label='Perfect Prediction')\n",
        "\n",
        "            # Set labels and title with metrics\n",
        "            plt.xlabel(\"True Age (years)\", fontsize=12, fontweight='bold')\n",
        "            plt.ylabel(\"Predicted Age (years)\", fontsize=12, fontweight='bold')\n",
        "            plt.title(f\"MAE = {overall_mae:.2f} ± {mae_std:.2f} years\",\n",
        "                      fontsize=14, fontweight='bold', pad=15)\n",
        "\n",
        "            # Customize grid\n",
        "            plt.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "            # Add legend\n",
        "            plt.legend(frameon=True, facecolor='white', framealpha=1,\n",
        "                      edgecolor='none', loc='upper left')\n",
        "\n",
        "            # Set equal aspect ratio\n",
        "            plt.axis('equal')\n",
        "\n",
        "            # Adjust layout\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Customize spines\n",
        "            for spine in plt.gca().spines.values():\n",
        "                spine.set_linewidth(1.5)\n",
        "\n",
        "            # Save the plot\n",
        "            plt.savefig(\"final_regression_plot.png\", dpi=300, bbox_inches='tight')\n",
        "            run.log({\"final_regression_plot\": wandb.Image(\"final_regression_plot.png\")})\n",
        "            plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        run.log({\n",
        "            'experiment_error': str(e),\n",
        "            'experiment_status': 'failed'\n",
        "        })\n",
        "        print(f\"Fatal error in experiment: {str(e)}\")\n",
        "\n",
        "    finally:\n",
        "        run.finish()\n",
        "\n",
        "    return {\n",
        "        'overall_mae': overall_mae if 'overall_mae' in locals() else None,\n",
        "        'overall_r2': overall_r2 if 'overall_r2' in locals() else None,\n",
        "        'predictions_df': predictions_df if 'predictions_df' in locals() else None\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gjG_47F7Mu9"
      },
      "outputs": [],
      "source": [
        "# from _typeshed import TraceFunction\n",
        "if __name__ == \"__main__\":\n",
        "    # Define parameters\n",
        "    body_sites = ['skin']\n",
        "    num_layers = [1]\n",
        "    hidden_dims = [512]\n",
        "    batch_sizes = [4096]\n",
        "    learning_rates = [0.001]\n",
        "    weight_decays = [0.001]\n",
        "    test_splits = [0.2]\n",
        "    optimizers = [optim.AdamW]\n",
        "    n_splits=10\n",
        "\n",
        "    # Device configuration\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Nested loops for parameter search\n",
        "    for body_site in body_sites:\n",
        "        for num_layer in num_layers:\n",
        "            for hidden_dim in hidden_dims:\n",
        "                for batch_size in batch_sizes:\n",
        "                    for lr in learning_rates:\n",
        "                        for wd in weight_decays:\n",
        "                            for test_split in test_splits:\n",
        "                                for optum in optimizers:\n",
        "                                    print(f\"\\nTrying parameters: hidden_dim={hidden_dim}, batch_size={batch_size}, \"\n",
        "                                        f\"learning_rate={lr}, weight_decay={wd}\")\n",
        "\n",
        "                                    current_params = {\n",
        "                                        'hidden_dim': hidden_dim,\n",
        "                                        'batch_size': batch_size,\n",
        "                                        'num_layers': num_layer,\n",
        "                                        'learning_rate': lr,\n",
        "                                        'weight_decay': wd,\n",
        "                                        'num_epochs': 1000,\n",
        "                                        'optimizer': optum,\n",
        "                                        'body_site': body_site,\n",
        "                                        'test_split': test_split,\n",
        "                                        'normalize_X': False,\n",
        "                                        'normalize_y': False,\n",
        "                                    }\n",
        "\n",
        "\n",
        "                                    run_cv_experiment(current_params, n_splits=n_splits, device=device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "transformers",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
